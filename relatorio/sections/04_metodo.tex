\section{Metodologia}
\label{sec:metodo}

Para investigar a interseção entre subjetividade e detecção de autoria, propomos um \textit{framework} experimental composto por três etapas sequenciais: (1) Treinamento supervisionado de modelos de SD ($M_{SD}$); (2) Treinamento supervisionado de modelos de MGTD ($M_{MGTD}$); e (3) Inferência cruzada e análise de correlação através de um Índice de Subjetividade agregado. A fim de garantir a robustez das observações e mitigar vieses de uma arquitetura específica, este experimento foi replicado utilizando três arquiteturas de base distintas, variando em tamanho e estratégia de pré-treinamento.

\subsection{Estratégia de Agregação e Índice de Subjetividade}
\label{ssec:agregacao}

Conforme discutido na Seção \ref{ssec:granularidade}, existe uma discrepância de granularidade entre os modelos. Para correlacionar as tarefas, formalizamos o seguinte processo de inferência em dois estágios:

Seja $D$ um documento do \textit{dataset} de MGTD. Primeiramente, aplicamos uma segmentação de sentenças $S_{seg}$, tal que $D = \{s_1, s_2, ..., s_n\}$.
O modelo de subjetividade $M_{SD}$ processa cada sentença $s_i$, resultando em uma probabilidade de subjetividade $P(subj|s_i)$. Definimos a função indicadora de classe $C(s_i)$ e o índice de subjetividade do documento $I_{subj}(D)$ como:

\begin{equation}
    I_{subj}(D) = \frac{1}{n} \sum_{i=1}^{n} P(subj | s_i)
\end{equation}

Onde $n$ é o número total de sentenças no documento. O $I_{subj}(D) \in [0, 1]$ representa a densidade média de subjetividade do documento.

\subsection{Arquiteturas Avaliadas}
\label{ssec:arquiteturas}

\subsubsection{Arquitetura A: BERT-Base com LoRA}
\label{ssec:metodo_bert}

Nesta primeira arquitetura adotamos o modelo relativamnete simples \textit{bert-base-uncased}. Para otimizar o custo computacional, implementamos a técnica de \textit{Low-Rank Adaptation} (LoRA). Em vez de ajustar todos os milhões de parâmetros do modelo, injetamos matrizes de decomposição de baixo posto nas camadas de atenção, treinando apenas estes adaptadores e a camada de classificação (\textit{classification head}).

Após a conclusão do treinamento, realizamos a operação de \textit{merge} dos pesos dos adaptadores LoRA diretamente nas matrizes do modelo base. Este procedimento resultou em um modelo unificado, contendo tanto o conhecimento pré-treinado quanto o ajuste fino, que foi salvo (incluindo a \textit{classification head}) para as etapas posteriores. O treinamento diferenciou-se para cada tarefa devido às características dos dados:

\begin{itemize}
    \item \textbf{Tarefa 1 (SD):} O modelo foi treinado no conjunto de treino e validado no conjunto de desenvolvimento. Definimos um total de 5 épocas, ponto onde observou-se o início de \textit{overfitting} (aumento da perda na validação). O modelo parcial foi utilizado junto do conjunto de testes para gerar os resultados do modelo $M_{SD}$.
    \item \textbf{Tarefa 2 (MGTD):} Dado o volume significativamente maior do \textit{dataset}, o treinamento foi restrito a 1 época completa. Esta decisão foi tomada para equilibrar o tempo de computação com a convergência do modelo. A \textit{evaluation strategy} e \textit{save strategy} foram definidas em termos de 1000 passos devido ao tamanho da época.
\end{itemize}

Para a etapa crítica de análise, onde as sentenças do dataset de MGTD são classificadas quanto à sua subjetividade, adotamos uma estratégia de treinar uma versão final do modelo de subjetividade utilizando a totalidade dos dados (agregando treino, validação e teste). Mantivemos os mesmos hiperparâmetros (5 épocas, LoRA) para garantir que este modelo responsável por gerar o Índice de Subjetividade ($I_{subj}$) possuísse a máxima capacidade de generalização possível.

\subsubsection{Distilbert}
\label{ssec:distilbert}
placeholder

\subsubsection{Arquitetura C: RoBERTa}
\label{ssec:Roberta}

Nesta arquitetura, adotamos o modelo \texttt{roberta-base}, uma variação robusta do BERT otimizada com um pré-treinamento mais extenso e remoção da tarefa de previsão de próxima sentença (NSP). Optamos pela estratégia de ajuste fino completo. Para viabilizar o custo computacional desta abordagem e acelerar o treinamento sem perda de performance, utilizamos a técnica de precisão mista. Adaptamos o fluxo de treinamento para cada tarefa da seguinte forma.

\begin{itemize}
    \item \textbf{Tarefa 1 (SD):} O treinamento foi monitorado utilizando a métrica F1-Macro no conjunto de validação. Devido à rápida convergência do modelo e à tendência de \textit{overfitting} observada a partir da 4ª época, implementamos a técnica de parada antecipada com paciência de 2 épocas. O \textit{Trainer} foi configurado para carregar automaticamente o melhor modelo ao final do processo, garantindo a máxima capacidade de generalização.

    \item \textbf{Tarefa 2 (MGTD):} Para a tarefa de detecção, realizamos o treinamento completo por uma época apenas, uma vez que o \textit{dataset} é muito grande. Utilizamos uma divisão estratégica dos dados, mantendo o conjunto de teste oficial da SemEval (Gold Labels) estritamente isolado para a avaliação final, evitando vazamento de dados (\textit{data leakage}).
\end{itemize}

Fizemos uma análise cruzada utilizando o modelo final da Tarefa 1 em modo de inferência para ``anotar'' todo o dataset da Tarefa 2. Cada documento do corpus de detecção recebeu um Score Médio de Subjetividade, agregado a partir das sentenças, permitindo a investigação da correlação entre a subjetividade do texto e os erros de classificação do detector.
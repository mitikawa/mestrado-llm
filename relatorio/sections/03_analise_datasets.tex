\section{Análise das Bases de Dados}
\label{sec:analise_datasets}

Esta secção detalha os dois \textit{datasets} utilizados, um para cada tarefa, e articula o desafio metodológico imposto pela sua diferença de granularidade.

\subsection{Tarefa 1}
\label{ssec:dataset_sd}

Para a tarefa 1, o \textit{dataset} é o \textbf{NewsSD-ENG} \cite{antici2024corpussentencelevelsubjectivitydetection}. Este é um corpus focado em notícias de língua inglesa que abordam tópicos controversos. O NewsSD-ENG fornece anotação explícita em nível de sentença.

O \textit{dataset} é composto por 1.049 sentenças, com as estatísticas de partição detalhadas na Tabela \ref{tab:stats_newssd}.

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Partição} & \textbf{\# Artigos} & \textbf{\# Sentenças} & \textbf{OBJ} & \textbf{SUBJ} \\
\midrule
Treino & 16 & 731 & 487 (12) & 244 (46) \\
Dev & 3 & 99 & 45 (3) & 54 (8) \\
Teste & 4 & 219 & 106 (4) & 113 (16) \\
\midrule
\textbf{Total} & \textbf{23} & \textbf{1.049} & \textbf{638 (19)} & \textbf{411 (70)} \\
\bottomrule
\end{tabular}
\caption{Estatísticas do corpus NewsSD-ENG, adaptado de \citet{antici2024corpussentencelevelsubjectivitydetection}. Em parênteses o número de sentenças disputadas (aquelas em que dois anotadores não concordaram e um terceiro fez a anotação).}
\label{tab:stats_newssd}
\end{table}

A análise da Tabela \ref{tab:stats_newssd} revela duas observações metodologicamente críticas que informam o nosso desenho experimental (Secção \ref{sec:metodo}):
\begin{enumerate}
    \item \textbf{Tamanho do Dataset:} O conjunto de treino é notavelmente reduzido (n=731). Esta escassez de dados impõe um risco significativo de \textit{overfitting} para LLMs submetidos a \textit{fine-tuning} completo.
    \item \textbf{Desbalanceamento de Classes:} O conjunto de treino exibe um desbalanceamento considerável (proporção de aprox. 2:1) em favor da classe objetiva (\texttt{OBJ}).
\end{enumerate}

Ambos os fatores justificam a adoção de uma abordagem de \textit{Parameter-Efficient Fine-Tuning} (PEFT), como o LoRA, e a utilização de estratégias de mitigação de desbalanceamento, como a ponderação de classes na função de perda (\textit{weighted loss}).

\subsection{Tarefa 2}
\label{ssec:dataset_mgtd}

Para a tarefa de MGTD, utilizamos os dados da \textbf{SemEval-2024 Task 8} \cite{wang-etal-2024-semeval-2024}. Em linha com o escopo do nosso trabalho, focamo-nos na \textbf{Subtarefa A} (classificação binária) e na trilha \textbf{monolíngue em inglês}.

Os dados são fornecidos em formato JSONL, divididos nas partições de treino e desenvolvimento (\textit{dev}). Cada instância no \textit{dataset} é composta pelo texto integral (\texttt{text}), o rótulo de origem (\texttt{label}, indicando \textit{human} ou \textit{machine}), o modelo gerador (ex: \texttt{GPT-4}, \texttt{Llama}, etc.), a fonte (\texttt{source}) e um identificador (\texttt{id}).

\subsection{Desafio de Granularidade dos Dados}
\label{ssec:granularidade}

Uma observação fundamental é a disparidade na granularidade dos dados entre as duas tarefas. A Tarefa 1 (SD) opera em \textbf{nível de sentença}. Em contrapartida, a Tarefa 2 (MGTD) opera em \textbf{nível de documento}, onde cada instância (\texttt{text}) consiste em múltiplos parágrafos.

Esta discrepância dificulta uma comparação direta e constitui um desafio metodológico central. Aplicar diretamente o classificador $M_{MGTD}$ (treinado em documentos) a uma sentença, ou aplicar o $M_{SD}$ (treinado em sentenças) diretamente a um documento $D_{MGTD}$, levaria a uma perda de performance.

Para superar este desafio, a nossa metodologia (detalhada na Secção \ref{sec:metodo}) emprega uma \textbf{estratégia de agregação}, onde o classificador $M_{CS}$ é aplicado iterativamente às sentenças de um documento $D_{MGTD}$ para calcular um "Índice de Subjetividade" agregado.